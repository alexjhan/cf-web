"""
Limpiador Mejorado - MÃ¡s agresivo en la agrupaciÃ³n de contenido
"""

import json
import re
from typing import List, Dict, Any

class ImprovedDocumentCleaner:
    
    def __init__(self):
        self.article_pattern = re.compile(r'^(ArtÃ­culo|Art\.?)\s*\d+[Â°Âº]?\s*[.-]?', re.IGNORECASE)
        self.chapter_pattern = re.compile(r'^(CAPÃTULO|CapÃ­tulo|TÃTULO|TÃ­tulo)\s*[IVX\d]+', re.IGNORECASE)
        
    def clean_document(self, json_file_path: str) -> Dict[str, Any]:
        """Limpia un documento con agrupaciÃ³n mÃ¡s inteligente"""
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        # Filtrar elementos vacÃ­os
        content_items = [item for item in raw_data['content'] 
                        if item['text'].strip() and len(item['text'].strip()) > 1]
        
        # Agrupar de manera mÃ¡s agresiva
        grouped_content = self._smart_grouping(content_items)
        
        # Crear chunks finales
        final_chunks = self._create_final_chunks(grouped_content)
        
        return {
            "source_file": raw_data['file'],
            "total_chunks": len(final_chunks),
            "chunks": final_chunks,
            "metadata": {
                "original_spans": len(raw_data['content']),
                "filtered_spans": len(content_items),
                "grouped_paragraphs": len(grouped_content),
                "final_chunks": len(final_chunks)
            }
        }
    
    def _smart_grouping(self, content_items: List[Dict]) -> List[Dict]:
        """Agrupa contenido de manera inteligente"""
        
        groups = []
        current_group = {
            "text": "",
            "type": "content",
            "spans": []
        }
        
        i = 0
        while i < len(content_items):
            item = content_items[i]
            text = item['text'].strip()
            
            # Detectar inicio de nueva secciÃ³n importante
            is_new_section = (
                self._is_major_title(text, item) or
                self._is_article_start(text) or
                self._is_chapter_start(text)
            )
            
            # Si encontramos nueva secciÃ³n y ya tenemos contenido
            if is_new_section and current_group['text'].strip():
                # Guardar grupo actual
                current_group['text'] = current_group['text'].strip()
                if current_group['text']:
                    groups.append(current_group)
                
                # Nuevo grupo
                current_group = {
                    "text": text,
                    "type": self._classify_content(text),
                    "spans": [item]
                }
            else:
                # Continuar agrupando
                if current_group['text'] and not current_group['text'].endswith(' '):
                    current_group['text'] += ' '
                current_group['text'] += text
                current_group['spans'].append(item)
            
            # Para artÃ­culos, agrupar las siguientes lÃ­neas hasta el prÃ³ximo artÃ­culo
            if self._is_article_start(text):
                current_group['type'] = 'article'
                # Buscar contenido del artÃ­culo
                i += 1
                while i < len(content_items):
                    next_item = content_items[i]
                    next_text = next_item['text'].strip()
                    
                    # Parar si encontramos otro artÃ­culo o secciÃ³n mayor
                    if (self._is_article_start(next_text) or 
                        self._is_chapter_start(next_text) or
                        self._is_major_title(next_text, next_item)):
                        i -= 1  # Retroceder para procesar en la prÃ³xima iteraciÃ³n
                        break
                    
                    # Agregar al artÃ­culo actual
                    if current_group['text'] and not current_group['text'].endswith(' '):
                        current_group['text'] += ' '
                    current_group['text'] += next_text
                    current_group['spans'].append(next_item)
                    i += 1
            
            i += 1
        
        # Agregar Ãºltimo grupo
        if current_group['text'].strip():
            current_group['text'] = current_group['text'].strip()
            groups.append(current_group)
        
        return groups
    
    def _is_major_title(self, text: str, item: Dict) -> bool:
        """Detecta tÃ­tulos principales"""
        return (
            (len(text) < 100 and text.isupper()) or
            item.get('size', 0) > 16 or
            self.chapter_pattern.match(text)
        )
    
    def _is_article_start(self, text: str) -> bool:
        """Detecta inicio de artÃ­culo"""
        return bool(self.article_pattern.match(text))
    
    def _is_chapter_start(self, text: str) -> bool:
        """Detecta inicio de capÃ­tulo"""
        return bool(self.chapter_pattern.match(text))
    
    def _classify_content(self, text: str) -> str:
        """Clasifica el tipo de contenido"""
        if self._is_chapter_start(text):
            return 'chapter'
        elif self._is_article_start(text):
            return 'article'
        elif len(text) < 100 and text.isupper():
            return 'title'
        else:
            return 'content'
    
    def _create_final_chunks(self, grouped_content: List[Dict]) -> List[Dict]:
        """Crea chunks finales optimizados para RAG"""
        
        chunks = []
        current_chunk = {
            "content": "",
            "title": "",
            "type": "content",
            "word_count": 0
        }
        
        for group in grouped_content:
            text = group['text']
            content_type = group['type']
            word_count = len(text.split())
            
            # Si es un tÃ­tulo o capÃ­tulo, empezar nuevo chunk
            if content_type in ['title', 'chapter']:
                # Guardar chunk anterior si tiene contenido sustancial
                if current_chunk['content'].strip() and current_chunk['word_count'] > 10:
                    chunks.append(current_chunk.copy())
                
                # Nuevo chunk
                current_chunk = {
                    "content": text,
                    "title": text,
                    "type": content_type,
                    "word_count": word_count
                }
            else:
                # Agregar al chunk actual
                if current_chunk['content']:
                    current_chunk['content'] += '\n\n' + text
                else:
                    current_chunk['content'] = text
                
                current_chunk['word_count'] += word_count
                
                # Si el chunk es muy largo, dividirlo (manteniendo el tÃ­tulo)
                if current_chunk['word_count'] > 600:
                    chunks.append(current_chunk.copy())
                    current_chunk = {
                        "content": "",
                        "title": current_chunk['title'],
                        "type": "content",
                        "word_count": 0
                    }
        
        # Agregar Ãºltimo chunk
        if current_chunk['content'].strip() and current_chunk['word_count'] > 10:
            chunks.append(current_chunk)
        
        return chunks

def main():
    """Procesar documentos con el limpiador mejorado"""
    import os
    
    cleaner = ImprovedDocumentCleaner()
    
    # Crear carpeta de salida
    os.makedirs("output", exist_ok=True)
    
    # Procesar documentos
    input_folder = "../data_ingest/output"
    processed_files = [f for f in os.listdir(input_folder) if f.endswith("_processed.json")]
    
    for json_filename in processed_files:
        json_file = os.path.join(input_folder, json_filename)
        print(f"\nğŸ”„ Procesando: {json_filename}")
        
        try:
            cleaned_doc = cleaner.clean_document(json_file)
            
            # Guardar resultado
            output_filename = json_filename.replace("_processed.json", "_improved_cleaned.json")
            output_file = os.path.join("output", output_filename)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(cleaned_doc, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Guardado: {output_file}")
            print(f"   ğŸ“Š Chunks: {cleaned_doc['total_chunks']}")
            print(f"   ğŸ“ˆ Spans: {cleaned_doc['metadata']['original_spans']} â†’ {cleaned_doc['metadata']['final_chunks']}")
            
            # Mostrar ejemplo del ArtÃ­culo 97Â°
            if "Estatuto" in json_filename:
                print(f"\nğŸ” Buscando ArtÃ­culo 97Â°...")
                for i, chunk in enumerate(cleaned_doc['chunks']):
                    if 'ArtÃ­culo 97' in chunk['content']:
                        print(f"âœ… Encontrado en chunk {i+1}:")
                        print(f"TÃ­tulo: {chunk['title']}")
                        print(f"Contenido: {chunk['content'][:300]}...")
                        break
                        
        except Exception as e:
            print(f"âŒ Error: {e}")
    
    print(f"\nğŸ‰ Â¡Procesamiento mejorado completado!")

if __name__ == "__main__":
    main()
